"""
Gradio GUI for Real-time AI Voice Translator
Provides a web-based interface for configuration and monitoring
"""
import gradio as gr
import numpy as np
import time
import os
from typing import Optional, Tuple, List
from pathlib import Path
from threading import Thread, Event
from queue import Queue
from rich.console import Console

# Import our modules
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import AppConfig, get_default_config
from modules.audio_io import AudioIO, AudioDevice
from modules.vad_asr import VADASR, ASRResult
from modules.translator import StreamingTranslator, TranslationChunk
from modules.sentence_buffer import SentenceBuffer, BufferChunk
from modules.tts_engine import TTSEngine, TTSChunk

console = Console()


class TranslatorApp:
    """Main application class that orchestrates all modules"""
    
    def __init__(self):
        self.config = get_default_config()
        
        # Module instances (lazy loaded)
        self._audio_io: Optional[AudioIO] = None
        self._vad_asr: Optional[VADASR] = None
        self._translator: Optional[StreamingTranslator] = None
        self._sentence_buffer: Optional[SentenceBuffer] = None
        self._tts_engine: Optional[TTSEngine] = None
        
        # State
        self._is_running = False
        self._stop_event = Event()
        self._processing_thread: Optional[Thread] = None
        
        # Logs and status
        self._asr_log: List[str] = []
        self._translation_log: List[str] = []
        self._latency_log: List[float] = []
    
    def get_audio_devices(self) -> Tuple[List[str], List[str]]:
        """Get lists of input and output audio devices"""
        input_devices = ["Default"] + [
            f"[{d.index}] {d.name}" for d in AudioIO.get_input_devices()
        ]
        output_devices = ["Default"] + [
            f"[{d.index}] {d.name}" for d in AudioIO.get_output_devices()
        ]
        return input_devices, output_devices
    
    def _parse_device_index(self, device_str: str) -> Optional[int]:
        """Parse device index from dropdown string"""
        if device_str == "Default" or not device_str:
            return None
        try:
            # Extract index from "[index] name"
            return int(device_str.split("]")[0].replace("[", ""))
        except:
            return None
    
    def initialize_modules(
        self,
        input_device: str,
        output_device: str,
        monitor_device: str,
        llm_provider: str,
        api_key: str,
        voice_name: str,
    ) -> str:
        """Initialize all modules with given configuration"""
        try:
            # Parse device indices
            input_idx = self._parse_device_index(input_device)
            output_idx = self._parse_device_index(output_device)
            monitor_idx = self._parse_device_index(monitor_device)
            
            # Initialize Audio I/O
            self._audio_io = AudioIO(
                input_sample_rate=16000,
                output_sample_rate=24000,
            )
            self._audio_io.set_input_device(input_idx)
            self._audio_io.set_output_device(output_idx)
            if monitor_idx is not None:
                self._audio_io.set_monitor_device(monitor_idx)
            
            # Initialize VAD + ASR
            self._vad_asr = VADASR(
                asr_model_size=self.config.asr.model_size,
                asr_device=self.config.asr.device,
                asr_compute_type=self.config.asr.compute_type,
                asr_language=self.config.asr.language,
            )
            
            # Initialize Translator
            if not api_key:
                return "âŒ Error: API Key is required"
            
            self._translator = StreamingTranslator(
                provider=llm_provider.lower(),
                api_key=api_key,
                source_language=self.config.translator.source_language,
                target_language=self.config.translator.target_language,
            )
            
            # Initialize Sentence Buffer
            self._sentence_buffer = SentenceBuffer(
                min_words_for_clause_cut=self.config.buffer.min_words_for_clause_cut,
                max_words_before_force_cut=self.config.buffer.max_words_before_force_cut,
                target_language="en",
            )
            
            # Initialize TTS Engine with selected voice
            self._tts_engine = TTSEngine(
                device=self.config.tts.device,
                sample_rate=self.config.tts.sample_rate,
                speaker=voice_name,
            )
            
            return f"âœ… åˆå§‹åŒ–å®Œæˆï¼èªéŸ³: {voice_name}"
            
        except Exception as e:
            return f"âŒ Error initializing modules: {str(e)}"
    
    def _pipeline_loop(self):
        """Main processing pipeline loop"""
        console.print("[green]Pipeline started[/green]")
        
        while not self._stop_event.is_set():
            try:
                # Get ASR result
                asr_result = self._vad_asr.get_result(timeout=0.1)
                
                if asr_result and asr_result.text.strip():
                    start_time = time.time()
                    
                    # DEBUG: Log ASR result
                    console.print(f"[cyan][DEBUG ASR] è­˜åˆ¥çµæœ: '{asr_result.text}'[/cyan]")
                    
                    # Log ASR result
                    self._asr_log.append(f"[{time.strftime('%H:%M:%S')}] {asr_result.text}")
                    
                    # Translate with streaming
                    full_translation = ""
                    try:
                        console.print(f"[yellow][DEBUG ç¿»è­¯] é–‹å§‹ç¿»è­¯...[/yellow]")
                        for trans_chunk in self._translator.translate_stream(asr_result.text):
                            if not trans_chunk.is_complete:
                                full_translation += trans_chunk.text
                                console.print(f"[yellow][DEBUG ç¿»è­¯] æ”¶åˆ°ç‰‡æ®µ: '{trans_chunk.text}'[/yellow]")
                                
                                # Feed to sentence buffer
                                buffer_chunks = self._sentence_buffer.feed(trans_chunk.text)
                                
                                # Send buffer chunks to TTS (only if text is not empty)
                                for buf_chunk in buffer_chunks:
                                    if buf_chunk.text and buf_chunk.text.strip():
                                        console.print(f"[magenta][DEBUG TTS] æ’å…¥åˆæˆä½‡åˆ—: '{buf_chunk.text}'[/magenta]")
                                        self._tts_engine.queue_synthesis(
                                            buf_chunk.text,
                                            buf_chunk.chunk_index,
                                            buf_chunk.is_final,
                                        )
                        
                        console.print(f"[green][DEBUG ç¿»è­¯] å®Œæ•´ç¿»è­¯: '{full_translation}'[/green]")
                        
                    except Exception as trans_error:
                        console.print(f"[red][DEBUG ç¿»è­¯] ç¿»è­¯éŒ¯èª¤: {trans_error}[/red]")
                        continue
                    
                    # Flush remaining buffer (only if we got a translation)
                    if full_translation.strip():
                        final_chunk = self._sentence_buffer.flush()
                        if final_chunk and final_chunk.text and final_chunk.text.strip():
                            console.print(f"[magenta][DEBUG TTS] æ’å…¥æœ€çµ‚ç‰‡æ®µ: '{final_chunk.text}'[/magenta]")
                            self._tts_engine.queue_synthesis(
                                final_chunk.text,
                                final_chunk.chunk_index,
                                True,
                            )
                        
                        # Log translation
                        self._translation_log.append(
                            f"[{time.strftime('%H:%M:%S')}] {full_translation}"
                        )
                        
                        # Calculate latency
                        latency = (time.time() - start_time) * 1000
                        self._latency_log.append(latency)
                        console.print(f"[blue][DEBUG å»¶é²] ç¸½å»¶é²: {latency:.0f}ms[/blue]")
                    else:
                        console.print(f"[red][DEBUG ç¿»è­¯] ç¿»è­¯çµæœç‚ºç©º![/red]")
                
                # Get TTS audio and play
                tts_chunk = self._tts_engine.get_audio_chunk(timeout=0.05)
                if tts_chunk:
                    audio = tts_chunk.audio
                    # Ensure 1D array
                    if audio.ndim > 1:
                        audio = audio.flatten()
                    
                    if len(audio) > 0:
                        duration_sec = len(audio) / tts_chunk.sample_rate
                        console.print(f"[green][DEBUG TTS] æ’­æ”¾éŸ³è¨Š: {len(audio)} samples ({duration_sec:.2f}ç§’), text='{tts_chunk.text[:30]}...'[/green]")
                        self._audio_io.play_audio(audio)
                    else:
                        console.print(f"[red][DEBUG TTS] æ”¶åˆ°ç©ºéŸ³è¨Š! text='{tts_chunk.text}'[/red]")
                
            except Exception as e:
                console.print(f"[red]Pipeline error: {e}[/red]")
        
        console.print("[yellow]Pipeline stopped[/yellow]")
    
    def start(self) -> str:
        """Start the translation pipeline"""
        if self._is_running:
            return "âš ï¸ Already running"
        
        if not all([self._audio_io, self._vad_asr, self._translator, self._tts_engine]):
            return "âŒ Please initialize modules first"
        
        try:
            # Clear logs
            self._asr_log = []
            self._translation_log = []
            self._latency_log = []
            
            # Set up audio input callback BEFORE starting streams
            def on_audio_input(audio_chunk):
                self._vad_asr.feed_audio(audio_chunk)
            
            self._audio_io.set_on_audio_input(on_audio_input)
            
            # Start all modules
            self._audio_io.start()
            self._vad_asr.start()
            self._tts_engine.start()
            
            # Start pipeline thread
            self._stop_event.clear()
            self._processing_thread = Thread(target=self._pipeline_loop, daemon=True)
            self._processing_thread.start()
            
            self._is_running = True
            return "ğŸ™ï¸ Translation started! Speak into your microphone..."
            
        except Exception as e:
            return f"âŒ Error starting: {str(e)}"
    
    def stop(self) -> str:
        """Stop the translation pipeline"""
        if not self._is_running:
            return "âš ï¸ Not running"
        
        try:
            self._stop_event.set()
            
            if self._audio_io:
                self._audio_io.stop()
            if self._vad_asr:
                self._vad_asr.stop()
            if self._tts_engine:
                self._tts_engine.stop()
            
            if self._processing_thread:
                self._processing_thread.join(timeout=2.0)
            
            self._is_running = False
            return "â¹ï¸ Translation stopped"
            
        except Exception as e:
            return f"âŒ Error stopping: {str(e)}"
    
    def get_status(self) -> Tuple[str, str, str]:
        """Get current status for display"""
        # ASR log - show recent entries
        if self._asr_log:
            asr_text = "\n".join(self._asr_log[-15:])
        else:
            asr_text = "ğŸ¤ ç­‰å¾…èªéŸ³è¼¸å…¥..."
        
        # Translation log - show recent entries
        if self._translation_log:
            trans_text = "\n".join(self._translation_log[-15:])
        else:
            trans_text = "ğŸŒ ç­‰å¾…ç¿»è­¯çµæœ..."
        
        # Latency info with more detail
        if self._latency_log:
            recent = self._latency_log[-10:]
            avg_latency = sum(recent) / len(recent)
            min_latency = min(recent)
            max_latency = max(recent)
            latency_text = f"å¹³å‡: {avg_latency:.0f}ms | æœ€å°: {min_latency:.0f}ms | æœ€å¤§: {max_latency:.0f}ms | æ¨£æœ¬æ•¸: {len(self._latency_log)}"
        else:
            latency_text = "â³ ç­‰å¾…æ•¸æ“š..."
        
        return asr_text, trans_text, latency_text


# Global app instance
app = TranslatorApp()


def create_app() -> gr.Blocks:
    """Create the Gradio interface"""
    
    # Get device lists
    input_devices, output_devices = app.get_audio_devices()
    
    with gr.Blocks() as demo:
        
        gr.Markdown("""
        # ğŸ™ï¸ MeetLingo
        
        **å³æ™‚èªéŸ³ç¿»è­¯** â€” å°ˆç‚ºç·šä¸Šæœƒè­°è¨­è¨ˆçš„é–‹æºè§£æ±ºæ–¹æ¡ˆ
        
        `Whisper ASR` â†’ `LLM ç¿»è­¯` â†’ `VibeVoice TTS` | å»¶é² < 1.5 ç§’
        
        **ä½¿ç”¨æµç¨‹**: âš™ï¸ è¨­å®š â†’ ğŸš€ åˆå§‹åŒ– â†’ ğŸ¤ é–‹å§‹ç¿»è­¯
        """)
        
        with gr.Tabs():
            # Tab 1: Configuration
            with gr.TabItem("âš™ï¸ è¨­å®š"):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### éŸ³è¨Šè¨­å‚™")
                        input_device = gr.Dropdown(
                            choices=input_devices,
                            value="Default",
                            label="è¼¸å…¥è¨­å‚™ (éº¥å…‹é¢¨)",
                        )
                        output_device = gr.Dropdown(
                            choices=output_devices,
                            value="Default",
                            label="è¼¸å‡ºè¨­å‚™ (VB-CABLE Input)",
                        )
                        monitor_device = gr.Dropdown(
                            choices=["None"] + output_devices,
                            value="None",
                            label="ç›£è½è¨­å‚™ (è€³æ©Ÿï¼Œå¯é¸)",
                        )
                        refresh_btn = gr.Button("ğŸ”„ é‡æ–°æ•´ç†è¨­å‚™")
                    
                    with gr.Column():
                        gr.Markdown("### LLM ç¿»è­¯è¨­å®š")
                        llm_provider = gr.Dropdown(
                            choices=["OpenAI", "Gemini", "Groq"],
                            value="OpenAI",
                            label="LLM æä¾›è€…",
                        )
                        api_key = gr.Textbox(
                            label="API Key",
                            type="password",
                            placeholder="è¼¸å…¥ä½ çš„ API Key",
                        )
                
                gr.Markdown("### ğŸ­ èªéŸ³è¨­å®š (VibeVoice)")
                with gr.Row():
                    voice_select = gr.Dropdown(
                        choices=[
                            ("Carter (ç”·, å°ˆæ¥­)", "en-Carter_man"),
                            ("Davis (ç”·, å¹´è¼•)", "en-Davis_man"),
                            ("Emma (å¥³, æº«æš–)", "en-Emma_woman"),
                            ("Frank (ç”·, æˆç†Ÿ)", "en-Frank_man"),
                            ("Grace (å¥³, å°ˆæ¥­)", "en-Grace_woman"),
                            ("Mike (ç”·, è¼•é¬†)", "en-Mike_man"),
                            ("Samuel (ç”·, å°åº¦è…”)", "in-Samuel_man"),
                        ],
                        value="en-Carter_man",
                        label="é¸æ“‡èªéŸ³",
                    )
                
                with gr.Row():
                    init_btn = gr.Button("ğŸš€ åˆå§‹åŒ–ç³»çµ±", variant="primary", size="lg")
                    init_status = gr.Textbox(label="åˆå§‹åŒ–ç‹€æ…‹", interactive=False)
            
            # Tab 2: Translation
            with gr.TabItem("ğŸ¤ ç¿»è­¯"):
                with gr.Row():
                    start_btn = gr.Button("â–¶ï¸ é–‹å§‹ç¿»è­¯", variant="primary", size="lg")
                    stop_btn = gr.Button("â¹ï¸ åœæ­¢ç¿»è­¯", variant="stop", size="lg")
                
                status_text = gr.Textbox(
                    label="ç‹€æ…‹",
                    interactive=False,
                    value="è«‹å…ˆåˆå§‹åŒ–ç³»çµ±",
                )
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### ğŸ“ èªéŸ³è­˜åˆ¥ (ASR)")
                        asr_output = gr.Textbox(
                            label="è­˜åˆ¥çµæœ",
                            lines=8,
                            interactive=False,
                            elem_classes=["log-box"],
                        )
                    
                    with gr.Column():
                        gr.Markdown("### ğŸŒ ç¿»è­¯çµæœ")
                        translation_output = gr.Textbox(
                            label="ç¿»è­¯æ–‡å­—",
                            lines=8,
                            interactive=False,
                            elem_classes=["log-box"],
                        )
                
                with gr.Row():
                    latency_display = gr.Textbox(
                        label="â±ï¸ å»¶é²ç›£æ§",
                        interactive=False,
                    )
                    refresh_status_btn = gr.Button("ğŸ”„ æ›´æ–°é¡¯ç¤º")
            
            # Tab 3: Help
            with gr.TabItem("â“ èªªæ˜"):
                gr.Markdown("""
                ## ğŸ™ï¸ MeetLingo
                
                å³æ™‚èªéŸ³ç¿»è­¯ï¼Œå°ˆç‚ºç·šä¸Šæœƒè­°è¨­è¨ˆçš„é–‹æºè§£æ±ºæ–¹æ¡ˆã€‚
                
                ---
                
                ### ğŸ“‹ å‰ç½®éœ€æ±‚
                
                | é …ç›® | èªªæ˜ |
                |------|------|
                | **VB-CABLE** | è™›æ“¬éŸ³æºç·šï¼Œ[ä¸‹è¼‰é€£çµ](https://vb-audio.com/Cable/) |
                | **API Key** | OpenAI / Gemini / Groq ä»»é¸ä¸€å€‹ |
                | **GPU** | NVIDIA RTX 3060+ (8GB VRAM) |
                | **VibeVoice èªéŸ³æª”** | `voices/streaming_model/*.pt` |
                
                ---
                
                ### âš™ï¸ è¨­å®šæ­¥é©Ÿ
                
                1. **é¸æ“‡è¼¸å…¥è¨­å‚™** â€” ä½ çš„éº¥å…‹é¢¨
                2. **é¸æ“‡è¼¸å‡ºè¨­å‚™** â€” `CABLE Input (VB-Audio Virtual Cable)`
                3. **è¼¸å…¥ API Key** â€” é¸æ“‡ LLM æä¾›è€…ä¸¦å¡«å…¥ Key
                4. **é»æ“Šã€Œåˆå§‹åŒ–ç³»çµ±ã€** â€” ç­‰å¾…æ¨¡å‹è¼‰å…¥å®Œæˆ
                
                ---
                
                ### ğŸ¯ æœƒè­°è»Ÿé«”è¨­å®š
                
                åœ¨ **Zoom / Teams / Meet** ä¸­ï¼š
                - éº¥å…‹é¢¨ï¼šé¸æ“‡ **`CABLE Output (VB-Audio Virtual Cable)`**
                
                é€™æ¨£æœƒè­°è»Ÿé«”æœƒæ¥æ”¶ç¿»è­¯å¾Œçš„è‹±æ–‡èªéŸ³ï¼
                
                ---
                
                ### ğŸš€ é–‹å§‹ä½¿ç”¨
                
                1. åˆ‡æ›åˆ°ã€Œç¿»è­¯ã€åˆ†é 
                2. é»æ“Š **ã€Œé–‹å§‹ç¿»è­¯ã€**
                3. å°è‘—éº¥å…‹é¢¨èªªä¸­æ–‡
                4. ç³»çµ±æœƒå³æ™‚ç¿»è­¯ä¸¦è¼¸å‡ºè‹±æ–‡èªéŸ³
                
                ---
                
                ### âš¡ å»¶é²å„ªåŒ–å»ºè­°
                
                | æ–¹æ³• | æ•ˆæœ |
                |------|------|
                | ä½¿ç”¨ **Groq API** | ç¿»è­¯é€Ÿåº¦æœ€å¿« (~300ms) |
                | ç¢ºä¿ **GPU å¯ç”¨** | ASR + TTS åŠ é€Ÿ |
                | èªªè©±æ™‚**åœé “æ¸…æ¥š** | å¹«åŠ© VAD åˆ‡åˆ† |
                
                ---
                
                ### ğŸ”§ å¸¸è¦‹å•é¡Œ
                
                **Q: æ²’æœ‰è²éŸ³è¼¸å‡ºï¼Ÿ**  
                A: ç¢ºèªé¸æ“‡äº†æ­£ç¢ºçš„è¼¸å‡ºè¨­å‚™ (CABLE Input)
                
                **Q: GPU è¨˜æ†¶é«”ä¸è¶³ï¼Ÿ**  
                A: é—œé–‰å…¶ä»– GPU ç¨‹å¼ï¼Œæˆ–ä½¿ç”¨è¼ƒå°çš„ Whisper æ¨¡å‹
                
                **Q: ç¿»è­¯å»¶é²å¤ªé«˜ï¼Ÿ**  
                A: å˜—è©¦ä½¿ç”¨ Groq APIï¼Œé€Ÿåº¦æœ€å¿«
                """)
        
        # Event handlers
        def refresh_devices():
            input_devs, output_devs = app.get_audio_devices()
            return (
                gr.update(choices=input_devs),
                gr.update(choices=output_devs),
                gr.update(choices=["None"] + output_devs),
            )
        
        refresh_btn.click(
            refresh_devices,
            outputs=[input_device, output_device, monitor_device],
        )
        
        def init_system(input_dev, output_dev, monitor_dev, provider, key, voice):
            return app.initialize_modules(
                input_dev, output_dev, monitor_dev,
                provider, key, voice,
            )
        
        init_btn.click(
            init_system,
            inputs=[
                input_device, output_device, monitor_device,
                llm_provider, api_key, voice_select,
            ],
            outputs=[init_status],
        )
        
        start_btn.click(app.start, outputs=[status_text])
        stop_btn.click(app.stop, outputs=[status_text])
        
        def update_status():
            asr, trans, latency = app.get_status()
            return asr, trans, latency
        
        refresh_status_btn.click(
            update_status,
            outputs=[asr_output, translation_output, latency_display],
        )
        
        # Auto-refresh using Timer (Gradio 4.x+)
        try:
            timer = gr.Timer(value=0.5, active=True)
            timer.tick(
                update_status,
                outputs=[asr_output, translation_output, latency_display],
            )
        except Exception:
            # Fallback for older Gradio: user needs to click refresh button
            console.print("[yellow]Auto-refresh not available, use manual refresh button[/yellow]")
        
    return demo


def launch_app(share: bool = False, server_port: int = 7860):
    """Launch the Gradio app"""
    demo = create_app()
    demo.launch(
        share=share,
        server_port=server_port,
        server_name="0.0.0.0",
    )


if __name__ == "__main__":
    launch_app()

